import asyncio
import json
import io
import re
from typing import Dict, Any, Optional, List, Union
import pandas as pd
import numpy as np
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import os
from dotenv import load_dotenv
import openpyxl
from scipy import stats
from collections import defaultdict
import openai

load_dotenv()

class AgentState:
    """Agent ìƒíƒœë¥¼ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤"""
    def __init__(self, **kwargs):
        self.query = kwargs.get("query", "")
        self.file_path = kwargs.get("file_path", "")
        self.analysis_type = kwargs.get("analysis_type", True)
        self.selected_question = kwargs.get("selected_question", "")
        self.selected_table = kwargs.get("selected_table", None)
        self.selected_key = kwargs.get("selected_key", "")
        self.anchor = kwargs.get("anchor", [])
        self.linearized_table = kwargs.get("linearized_table", "")
        self.numeric_analysis = kwargs.get("numeric_analysis", "")
        self.table_analysis = kwargs.get("table_analysis", "")
        self.revised_analysis = kwargs.get("revised_analysis", "")
        self.hallucination_check = kwargs.get("hallucination_check", "")
        self.hallucination_reject_num = kwargs.get("hallucination_reject_num", 0)
        self.feedback = kwargs.get("feedback", "")
        self.polishing_result = kwargs.get("polishing_result", "")
        self.generated_hypotheses = kwargs.get("generated_hypotheses", "")
        self.uploaded_file = kwargs.get("uploaded_file", None)
        self.raw_data_file = kwargs.get("raw_data_file", None)
        self.raw_data = kwargs.get("raw_data", None)
        self.raw_variables = kwargs.get("raw_variables", None)
        self.raw_code_guide = kwargs.get("raw_code_guide", None)
        self.raw_question = kwargs.get("raw_question", None)
        self.test_type = kwargs.get("test_type", None)
        self.ft_test_result = kwargs.get("ft_test_result", {})
        self.ft_test_summary = kwargs.get("ft_test_summary", "")
        self.ft_error = kwargs.get("ft_error", None)
        self.lang = kwargs.get("lang", "í•œêµ­ì–´")
        self.tables = kwargs.get("tables", {})
        self.question_texts = kwargs.get("question_texts", {})
        self.question_keys = kwargs.get("question_keys", [])
        self.revised_analysis_history = kwargs.get("revised_analysis_history", [])
        self.user_id = kwargs.get("user_id", None)
        self.survey_data = kwargs.get("survey_data", None)

class LangGraphWorkflow:
    def __init__(self):
        self.llm = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0.3,
            api_key=os.getenv("OPENAI_API_KEY")
        )
    
    def normalize_key(self, key: str) -> str:
        """í‚¤ ì •ê·œí™”"""
        return key.strip().replace(' ', '').replace('-', '').replace('_', '').replace('.', '').upper()
    
    def find_matching_key(self, target_key: str, available_keys: List[str]) -> Optional[str]:
        """í‚¤ ë§¤ì¹­ ì°¾ê¸°"""
        normalized_target = self.normalize_key(target_key)
        
        # ì •í™•í•œ ë§¤ì¹­
        for key in available_keys:
            if self.normalize_key(key) == normalized_target:
                return key
        
        # í¬í•¨ ê´€ê³„ ë§¤ì¹­
        for key in available_keys:
            normalized_key = self.normalize_key(key)
            if normalized_key in normalized_target or normalized_target in normalized_key:
                return key
        
        # ìœ ì‚¬ë„ ê¸°ë°˜ ë§¤ì¹­ (Levenshtein distance)
        best_match = None
        best_score = float('inf')
        
        for key in available_keys:
            normalized_key = self.normalize_key(key)
            distance = self.levenshtein_distance(normalized_target, normalized_key)
            max_length = max(len(normalized_target), len(normalized_key))
            similarity = distance / max_length
            
            if similarity < 0.3 and similarity < best_score:
                best_match = key
                best_score = similarity
        
        return best_match
    
    def levenshtein_distance(self, str1: str, str2: str) -> int:
        """Levenshtein ê±°ë¦¬ ê³„ì‚°"""
        matrix = [[0] * (len(str2) + 1) for _ in range(len(str1) + 1)]
        
        for i in range(len(str1) + 1):
            matrix[i][0] = i
        
        for j in range(len(str2) + 1):
            matrix[0][j] = j
        
        for i in range(1, len(str1) + 1):
            for j in range(1, len(str2) + 1):
                if str1[i-1] == str2[j-1]:
                    matrix[i][j] = matrix[i-1][j-1]
                else:
                    matrix[i][j] = min(
                        matrix[i-1][j-1] + 1,
                        matrix[i][j-1] + 1,
                        matrix[i-1][j] + 1
                    )
        
        return matrix[len(str1)][len(str2)]
    
    async def load_survey_tables(self, file_content: bytes, file_name: str, sheet_name: str = "í†µê³„í‘œ"):
        """ì„¤ë¬¸ í…Œì´ë¸” ë¡œë“œ - DataProcessorì˜ load_survey_tablesì™€ ë™ì¼í•˜ê²Œ ë™ì‘í•˜ë„ë¡ ìˆ˜ì •"""
        from utils.data_processor import DataProcessor
        data_processor = DataProcessor()
        return data_processor.load_survey_tables(file_content, file_name, sheet_name)
    
    def linearize_row_wise(self, table: pd.DataFrame) -> str:
        """í…Œì´ë¸”ì„ í–‰ ë‹¨ìœ„ë¡œ ì„ í˜•í™”"""
        if table is None or table.empty:
            return ""
        
        # ìˆ«ì ì»¬ëŸ¼ë§Œ ì„ íƒ
        numeric_cols = table.select_dtypes(include=[np.number]).columns.tolist()
        if not numeric_cols:
            return ""
        
        # ì²« ë²ˆì§¸ ì»¬ëŸ¼ (ê·¸ë£¹ëª…)ê³¼ ìˆ«ì ì»¬ëŸ¼ë“¤ë§Œ ì‚¬ìš©
        result_cols = [table.columns[0]] + numeric_cols
        result_df = table[result_cols]
        
        # í–‰ ë‹¨ìœ„ë¡œ ë¬¸ìì—´ ë³€í™˜
        rows = []
        for _, row in result_df.iterrows():
            row_str = " | ".join([str(val) for val in row.values])
            rows.append(row_str)
        
        return "\n".join(rows)
    
    async def make_openai_call(self, messages: List[Dict[str, str]], model: str = "gpt-4o-mini", temperature: float = 0.3) -> str:
        """OpenAI API í˜¸ì¶œ"""
        try:
            langchain_messages = []
            for msg in messages:
                if msg["role"] == "system":
                    langchain_messages.append(SystemMessage(content=msg["content"]))
                else:
                    langchain_messages.append(HumanMessage(content=msg["content"]))
            
            response = await self.llm.ainvoke(langchain_messages)
            return response.content.strip()
        except Exception as e:
            raise Exception(f"OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
    
    async def table_parser_node(self, state: AgentState, on_step=None) -> AgentState:
        """í…Œì´ë¸” íŒŒì„œ ë…¸ë“œ"""
        if on_step:
            on_step("ğŸ“Š í…Œì´ë¸” íŒŒì„œ ë…¸ë“œ ì‹œì‘")
        
        # íŒŒì¼ì—ì„œ í…Œì´ë¸” íŒŒì‹±
        if state.uploaded_file:
            parsed_data = await self.load_survey_tables(
                state.uploaded_file,
                state.file_path
            )
            state.tables = parsed_data["tables"]
            state.question_texts = parsed_data["question_texts"]
            state.question_keys = parsed_data["question_keys"]
        
        # ì„ íƒëœ í‚¤ ë§¤ì¹­
        if state.selected_key and state.tables:
            matching_key = self.find_matching_key(state.selected_key, list(state.tables.keys()))
            if matching_key:
                state.selected_key = matching_key
                state.selected_table = state.tables[matching_key]
                state.selected_question = state.question_texts[matching_key]
            elif state.analysis_type:
                raise Exception(f"ì„ íƒëœ ì§ˆë¬¸ í‚¤ '{state.selected_key}'ì— í•´ë‹¹í•˜ëŠ” í…Œì´ë¸”ì´ ì—†ìŠµë‹ˆë‹¤.")
            else:
                # ë°°ì¹˜ ë¶„ì„ì˜ ê²½ìš° ì²« ë²ˆì§¸ ì§ˆë¬¸ ì‚¬ìš©
                state.selected_key = state.question_keys[0]
                state.selected_table = state.tables[state.selected_key]
                state.selected_question = state.question_texts[state.selected_key]
        
        # í…Œì´ë¸” ì„ í˜•í™”
        if state.selected_table is not None:
            state.linearized_table = self.linearize_row_wise(state.selected_table)
        
        return state
    
    async def hypothesis_generate_node(self, state: AgentState, on_step=None) -> AgentState:
        """ê°€ì„¤ ìƒì„± ë…¸ë“œ"""
        if on_step:
            on_step("ğŸ’¡ ê°€ì„¤ ìƒì„± ë…¸ë“œ ì‹œì‘")
        
        prompt = f"""ë‹¤ìŒ ì„¤ë¬¸ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ í†µê³„ì  ê°€ì„¤ì„ ìƒì„±í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {state.selected_question}
ë°ì´í„°: {state.linearized_table}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ê°€ì„¤ì„ ìƒì„±í•´ì£¼ì„¸ìš”:
1. ì£¼ìš” ê°€ì„¤ (H1, H2, H3...)
2. í†µê³„ ê²€ì • ë°©ë²• ì œì•ˆ
3. ì˜ˆìƒ ê²°ê³¼

ì–¸ì–´: {state.lang}"""

        messages = [
            {"role": "system", "content": "ë‹¹ì‹ ì€ í†µê³„ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
            {"role": "user", "content": prompt}
        ]
        
        state.generated_hypotheses = await self.make_openai_call(messages)
        return state
    
    def rule_based_test_type_decision(self, columns, question_text=""):
        """
        ì»¬ëŸ¼ëª…ê³¼ ì§ˆë¬¸ í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„ì˜ ë¶„ì„/ft_test/chi_squareë¥¼ rule ê¸°ë°˜ìœ¼ë¡œ íŒë‹¨
        """
        import re

        # 1ï¸âƒ£ ì„ì˜ ë¶„ì„ íŒë‹¨: ë³µìˆ˜ ì‘ë‹µ ë˜ëŠ” ìˆœìœ„ ì‘ë‹µ íŒ¨í„´ ì¡´ì¬ ì—¬ë¶€
        multi_response_keywords = [
            "1+2", "1+2+3", "ë³µìˆ˜", "ë‹¤ì¤‘", "multiple", "rank", "ranking", "ìš°ì„ ìˆœìœ„"
        ]
        text_to_check = (" ".join(columns) + " " + question_text).lower()
        if any(keyword.lower() in text_to_check for keyword in multi_response_keywords):
            return "manual"

        # 2ï¸âƒ£ ft_test íŒë‹¨ ê¸°ì¤€: ì „í˜•ì ì¸ ë²”ì£¼í˜• í‘œí˜„ì´ ìˆëŠ” ê²½ìš°
        categorical_patterns = [
            # ê´€ì‹¬ë„ ê´€ë ¨
            r"ì „í˜€\s*ê´€ì‹¬", r"ê´€ì‹¬\s*ì—†(ë‹¤|ëŠ”)", r"ê´€ì‹¬\s*ìˆ(ë‹¤|ëŠ”)", r"ë§¤ìš°\s*ê´€ì‹¬", r"ê´€ì‹¬",

            # ë§Œì¡±ë„ ê´€ë ¨
            r"ë§¤ìš°\s*ë§Œì¡±", r"ë§Œì¡±", r"ë¶ˆë§Œì¡±", r"ë§¤ìš°\s*ë¶ˆë§Œì¡±", r"ë³´í†µ",

            # ì°¬ë°˜ ê´€ë ¨
            r"ì°¬ì„±", r"ë°˜ëŒ€", r"ë§¤ìš°\s*ì°¬ì„±", r"ë§¤ìš°\s*ë°˜ëŒ€", r"ëŒ€ì²´ë¡œ\s*ì°¬ì„±", r"ëŒ€ì²´ë¡œ\s*ë°˜ëŒ€",

            # ì¤‘ìš”ë„ ê´€ë ¨
            r"ë§¤ìš°\s*ì¤‘ìš”", r"ì¤‘ìš”", r"ê·¸ë‹¤ì§€\s*ì¤‘ìš”í•˜ì§€\s*ì•Š", r"ì „í˜€\s*ì¤‘ìš”í•˜ì§€\s*ì•Š",

            # ì‹¬ê°ì„± ì¸ì‹
            r"ë§¤ìš°\s*ì‹¬ê°", r"ì‹¬ê°", r"ì‹¬ê°í•˜ì§€\s*ì•Š", r"ì „í˜€\s*ì‹¬ê°í•˜ì§€\s*ì•Š",

            # ë¹ˆë„ ê´€ë ¨
            r"ìì£¼", r"ê°€ë”", r"ê±°ì˜\s*ì—†", r"ì „í˜€\s*ì—†",

            # ì•ˆì „ì„± ê´€ë ¨
            r"ì•ˆì „", r"ë§¤ìš°\s*ì•ˆì „", r"ìœ„í—˜", r"ë§¤ìš°\s*ìœ„í—˜",

            # ì¸ì§€/ê²½í—˜ ì—¬ë¶€
            r"ë“¤ì–´ë³¸\s*ì ", r"ì‚¬ìš©í•œ\s*ì ", r"ê²½í—˜í–ˆ", r"ì¸ì§€",

            # íƒœë„/ì˜í–¥ ê´€ë ¨
            r"ì˜í–¥", r"ìƒê°", r"ì˜ˆì •", r"ê³„íš", r"í• \s*ê²ƒ",

            # ì •ë„ í‘œí˜„
            r"ë§¤ìš°", r"ì•½ê°„", r"ë³´í†µ", r"ê·¸ë‹¤ì§€", r"ì „í˜€"
        ]
        if any(any(re.search(pattern, col) for pattern in categorical_patterns) for col in columns):
            return "ft_test"

        # 3ï¸âƒ£ ë‚˜ë¨¸ì§€ëŠ” chi_square
        return "chi_square"
    
    async def test_decision_node(self, state: AgentState, on_step=None) -> AgentState:
        """ê²€ì • ë°©ë²• ê²°ì • ë…¸ë“œ"""
        if on_step:
            on_step("ğŸ§­ í†µê³„ ê²€ì • ê²°ì • ë…¸ë“œ ì‹œì‘")
        if state.selected_table is not None:
            columns = state.selected_table.columns.tolist()
            # ë‹¨ì¼ ë¬¸í•­ ë¶„ì„ì´ë©´ LLM, ì¼ê´„ ë¶„ì„ì´ë©´ rule-based
            if getattr(state, 'analysis_type', True):
                # ë‹¨ì¼ ë¬¸í•­ ë¶„ì„: LLM
                from utils.data_processor import DataProcessor
                data_processor = DataProcessor()
                test_type = await data_processor.llm_test_type_decision(columns, state.selected_question)
            else:
                # ì¼ê´„ ë¶„ì„: rule-based
                from utils.data_processor import DataProcessor
                data_processor = DataProcessor()
                test_type = data_processor.rule_based_test_type_decision(columns, state.selected_question)
            state.test_type = test_type
        return state
    
    async def ft_analysis_node(self, state: AgentState, on_step=None) -> AgentState:
        """F/T ë¶„ì„ ë…¸ë“œ"""
        if on_step:
            on_step("âœ… F/T ë¶„ì„ ë…¸ë“œ ì‹œì‘")
        
        try:
            if state.selected_table is not None:
                # í†µê³„ ë¶„ì„ ì‹¤í–‰
                result = await self.run_statistical_tests(
                    state.test_type,
                    state.selected_table,
                    state.selected_key
                )
                state.ft_test_result = result
                
                # ê²°ê³¼ ìš”ì•½
                summary = self.summarize_ft_test(result, state.lang)
                state.ft_test_summary = summary
        except Exception as e:
            state.ft_error = str(e)
        
        return state
    
    async def run_statistical_tests(self, test_type: str, df: pd.DataFrame, question_key: str) -> List[Dict[str, Any]]:
        """í†µê³„ ê²€ì • ì‹¤í–‰"""
        results = []
        
        try:
            if test_type == "t-test":
                # t-test ì‹¤í–‰
                numeric_cols = df.select_dtypes(include=[np.number]).columns
                if len(numeric_cols) >= 2:
                    for i in range(len(numeric_cols)):
                        for j in range(i+1, len(numeric_cols)):
                            col1, col2 = numeric_cols[i], numeric_cols[j]
                            stat, p_value = stats.ttest_ind(df[col1].dropna(), df[col2].dropna())
                            results.append({
                                "test_type": "t-test",
                                "columns": [col1, col2],
                                "statistic": float(stat),
                                "p_value": float(p_value),
                                "significant": p_value < 0.05
                            })
            
            elif test_type == "chi-square":
                # chi-square test ì‹¤í–‰
                categorical_cols = df.select_dtypes(include=['object']).columns
                if len(categorical_cols) >= 2:
                    for i in range(len(categorical_cols)):
                        for j in range(i+1, len(categorical_cols)):
                            col1, col2 = categorical_cols[i], categorical_cols[j]
                            contingency_table = pd.crosstab(df[col1], df[col2])
                            chi2, p_value, dof, expected = stats.chi2_contingency(contingency_table)
                            results.append({
                                "test_type": "chi-square",
                                "columns": [col1, col2],
                                "statistic": float(chi2),
                                "p_value": float(p_value),
                                "significant": p_value < 0.05
                            })
            
            else:
                # ê¸°ë³¸ ê¸°ìˆ í†µê³„
                numeric_cols = df.select_dtypes(include=[np.number]).columns
                for col in numeric_cols:
                    stats_data = df[col].describe()
                    results.append({
                        "test_type": "descriptive",
                        "column": col,
                        "mean": float(stats_data['mean']),
                        "std": float(stats_data['std']),
                        "min": float(stats_data['min']),
                        "max": float(stats_data['max'])
                    })
        
        except Exception as e:
            results.append({
                "test_type": "error",
                "error": str(e)
            })
        
        return results
    
    def summarize_ft_test(self, results: List[Dict[str, Any]], lang: str = "í•œêµ­ì–´") -> str:
        """í†µê³„ ê²€ì • ê²°ê³¼ ìš”ì•½"""
        if not results:
            return "í†µê³„ ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        summary_parts = []
        
        for result in results:
            if result.get("test_type") == "t-test":
                col1, col2 = result["columns"]
                p_value = result["p_value"]
                significant = "ìœ ì˜í•¨" if result["significant"] else "ìœ ì˜í•˜ì§€ ì•ŠìŒ"
                summary_parts.append(f"t-test ({col1} vs {col2}): p={p_value:.4f} ({significant})")
            
            elif result.get("test_type") == "chi-square":
                col1, col2 = result["columns"]
                p_value = result["p_value"]
                significant = "ìœ ì˜í•¨" if result["significant"] else "ìœ ì˜í•˜ì§€ ì•ŠìŒ"
                summary_parts.append(f"Chi-square ({col1} vs {col2}): p={p_value:.4f} ({significant})")
            
            elif result.get("test_type") == "descriptive":
                col = result["column"]
                mean = result["mean"]
                std = result["std"]
                summary_parts.append(f"{col}: í‰ê· ={mean:.2f}, í‘œì¤€í¸ì°¨={std:.2f}")
        
        return "\n".join(summary_parts)
    
    async def get_anchor_node(self, state: AgentState, on_step=None) -> AgentState:
        """ì•µì»¤ ì¶”ì¶œ ë…¸ë“œ"""
        if on_step:
            on_step("ğŸ“Œ ì•µì»¤ ì¶”ì¶œ ë…¸ë“œ ì‹œì‘")
        
        prompt = f"""ë‹¤ìŒ ë°ì´í„°ì—ì„œ ì£¼ìš” ì•µì»¤ í¬ì¸íŠ¸ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

ë°ì´í„°: {state.linearized_table}
í†µê³„ ê²°ê³¼: {state.ft_test_summary}

ì£¼ìš” ì•µì»¤ í¬ì¸íŠ¸ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”."""

        messages = [
            {"role": "system", "content": "ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
            {"role": "user", "content": prompt}
        ]
        
        anchor_response = await self.make_openai_call(messages)
        state.anchor = [line.strip() for line in anchor_response.split('\n') if line.strip()]
        
        return state
    
    async def table_analyzer(self, state: AgentState, on_step=None) -> AgentState:
        """í…Œì´ë¸” ë¶„ì„ ë…¸ë“œ"""
        if on_step:
            on_step("ğŸ¤– í…Œì´ë¸” ë¶„ì„ ë…¸ë“œ ì‹œì‘")
        
        prompt = f"""ë‹¤ìŒ ë°ì´í„°ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {state.selected_question}
ë°ì´í„°: {state.linearized_table}
í†µê³„ ê²°ê³¼: {state.ft_test_summary}
ì•µì»¤ í¬ì¸íŠ¸: {', '.join(state.anchor)}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”:
1. ë°ì´í„° ê°œìš”
2. ì£¼ìš” ë°œê²¬ì‚¬í•­
3. í†µê³„ì  ì˜ë¯¸
4. ì‹¤ë¬´ì  í•¨ì˜

ì–¸ì–´: {state.lang}"""

        messages = [
            {"role": "system", "content": "ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
            {"role": "user", "content": prompt}
        ]
        
        state.table_analysis = await self.make_openai_call(messages)
        return state
    
    async def hallucination_check_node(self, state: AgentState, on_step=None) -> AgentState:
        """í™˜ê° ê²€ì¦ ë…¸ë“œ"""
        if on_step:
            on_step("ğŸ§  í™˜ê° í‰ê°€ ë…¸ë“œ ì‹œì‘")
        
        prompt = f"""ë‹¤ìŒ ë¶„ì„ ê²°ê³¼ê°€ ë°ì´í„°ì— ê·¼ê±°í•œ ê²ƒì¸ì§€ ê²€ì¦í•´ì£¼ì„¸ìš”.

ì›ë³¸ ë°ì´í„°: {state.linearized_table}
ë¶„ì„ ê²°ê³¼: {state.table_analysis}

ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¡œ ë‹µí•´ì£¼ì„¸ìš”:
- accept: ë°ì´í„°ì— ê·¼ê±°í•œ ë¶„ì„
- reject: ë°ì´í„°ì— ê·¼ê±°í•˜ì§€ ì•Šì€ ë¶„ì„

ì´ìœ ë„ í•¨ê»˜ ì„¤ëª…í•´ì£¼ì„¸ìš”."""

        messages = [
            {"role": "system", "content": "ë‹¹ì‹ ì€ ë°ì´í„° ê²€ì¦ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
            {"role": "user", "content": prompt}
        ]
        
        check_response = await self.make_openai_call(messages)
        
        if "accept" in check_response.lower():
            state.hallucination_check = "accept"
        elif "reject" in check_response.lower():
            state.hallucination_check = "reject"
        else:
            state.hallucination_check = "reject"  # ê¸°ë³¸ê°’
        
        state.feedback = check_response
        return state
    
    async def revise_table_analysis(self, state: AgentState, on_step=None) -> AgentState:
        """ë¶„ì„ ìˆ˜ì • ë…¸ë“œ"""
        if on_step:
            on_step("âœï¸ ë¶„ì„ ìˆ˜ì • ë…¸ë“œ ì‹œì‘")
        
        # ì´ì „ ë¶„ì„ì„ íˆìŠ¤í† ë¦¬ì— ì €ì¥
        if state.table_analysis:
            if not state.revised_analysis_history:
                state.revised_analysis_history = []
            state.revised_analysis_history.append(state.table_analysis)
        
        prompt = f"""ì´ì „ ë¶„ì„ì´ ê±°ë¶€ë˜ì—ˆìŠµë‹ˆë‹¤. ë°ì´í„°ì— ë” ê·¼ê±°í•œ ë¶„ì„ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

ì›ë³¸ ë°ì´í„°: {state.linearized_table}
í†µê³„ ê²°ê³¼: {state.ft_test_summary}
í”¼ë“œë°±: {state.feedback}

ë°ì´í„°ì— ì—„ê²©íˆ ê·¼ê±°í•˜ì—¬ ë‹¤ì‹œ ë¶„ì„í•´ì£¼ì„¸ìš”."""

        messages = [
            {"role": "system", "content": "ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë°ì´í„°ì— ì—„ê²©íˆ ê·¼ê±°í•˜ì—¬ ë¶„ì„í•˜ì„¸ìš”."},
            {"role": "user", "content": prompt}
        ]
        
        state.revised_analysis = await self.make_openai_call(messages)
        state.table_analysis = state.revised_analysis
        
        return state
    
    async def sentence_polish_node(self, state: AgentState, on_step=None) -> AgentState:
        """ë¬¸ì¥ ë‹¤ë“¬ê¸° ë…¸ë“œ"""
        if on_step:
            on_step("ğŸ’… ë¬¸ì¥ ë‹¤ë“¬ê¸° ë…¸ë“œ ì‹œì‘")
        
        prompt = f"""ë‹¤ìŒ ë¶„ì„ ê²°ê³¼ë¥¼ ë” ëª…í™•í•˜ê³  ì½ê¸° ì‰½ê²Œ ë‹¤ë“¬ì–´ì£¼ì„¸ìš”.

ë¶„ì„ ê²°ê³¼: {state.table_analysis}

ë‹¤ìŒ ìš”êµ¬ì‚¬í•­ì„ ë§Œì¡±í•˜ë„ë¡ ìˆ˜ì •í•´ì£¼ì„¸ìš”:
1. ëª…í™•í•˜ê³  ê°„ê²°í•œ ë¬¸ì¥
2. ë…¼ë¦¬ì  êµ¬ì¡°
3. ì‹¤ë¬´ìê°€ ì´í•´í•˜ê¸° ì‰¬ìš´ í‘œí˜„
4. êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ì™€ ê·¼ê±° í¬í•¨

ì–¸ì–´: {state.lang}"""

        messages = [
            {"role": "system", "content": "ë‹¹ì‹ ì€ ë¬¸ì„œ ì‘ì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
            {"role": "user", "content": prompt}
        ]
        
        state.polishing_result = await self.make_openai_call(messages)
        return state
    
    async def execute(self, file_content: bytes, file_name: str, options: Dict[str, Any] = None) -> Dict[str, Any]:
        """ì›Œí¬í”Œë¡œìš° ì‹¤í–‰"""
        try:
            # ì´ˆê¸° ìƒíƒœ ì„¤ì •
            state = AgentState(
                uploaded_file=file_content,
                file_path=file_name,
                analysis_type=options.get("analysis_type", True) if options else True,
                selected_key=options.get("selected_key", "") if options else "",
                lang=options.get("lang", "í•œêµ­ì–´") if options else "í•œêµ­ì–´",
                user_id=options.get("user_id") if options else None
            )
            
            on_step = options.get("on_step") if options else None
            
            # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
            state = await self.table_parser_node(state, on_step)
            state = await self.hypothesis_generate_node(state, on_step)
            state = await self.test_decision_node(state, on_step)
            state = await self.ft_analysis_node(state, on_step)
            state = await self.get_anchor_node(state, on_step)
            state = await self.table_analyzer(state, on_step)
            
            # í™˜ê° ê²€ì¦ ë° ìˆ˜ì • ë£¨í”„
            max_revisions = 4
            while state.hallucination_reject_num < max_revisions:
                state = await self.hallucination_check_node(state, on_step)
                if state.hallucination_check == "accept":
                    break
                elif state.hallucination_check == "reject":
                    if state.hallucination_reject_num >= 4:
                        if on_step:
                            on_step("âš ï¸ ê±°ë¶€ íšŸìˆ˜ ì´ˆê³¼, ì¢…ë£Œí•©ë‹ˆë‹¤.")
                        break
                    state = await self.revise_table_analysis(state, on_step)
                    state.hallucination_reject_num += 1
                else:
                    raise Exception(f"ì˜ˆìƒì¹˜ ëª»í•œ ê²°ì •: {state.hallucination_check}")
            
            state = await self.sentence_polish_node(state, on_step)
            
            return {
                "success": True,
                "result": {
                    "polishing_result": state.polishing_result,
                    "table_analysis": state.table_analysis,
                    "ft_test_summary": state.ft_test_summary,
                    "generated_hypotheses": state.generated_hypotheses,
                    "anchor": state.anchor,
                    "revised_analysis_history": state.revised_analysis_history
                }
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e)
            } 