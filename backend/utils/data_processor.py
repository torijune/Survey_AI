import os
import json
import pandas as pd
import numpy as np
from typing import Dict, Any, List, Optional
from docx import Document
import re
from io import BytesIO
from collections import defaultdict
import io
import openai

class DataProcessor:
    def __init__(self):
        pass
    
    async def run_statistical_test(
        self,
        file_content: bytes,
        file_name: str,
        test_type: str = "auto",
        question_key: str = ""
    ) -> Dict[str, Any]:
        """í†µê³„ ë¶„ì„ ì‹¤í–‰"""
        try:
            # íŒŒì¼ì„ DataFrameìœ¼ë¡œ ë¡œë“œ
            if file_name.endswith('.xlsx') or file_name.endswith('.xls'):
                df = pd.read_excel(io.BytesIO(file_content))
            elif file_name.endswith('.csv'):
                df = pd.read_csv(io.BytesIO(file_content))
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_name}")
            
            # ê¸°ë³¸ í†µê³„ ê³„ì‚°
            numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()
            
            stats = {
                "file_name": file_name,
                "test_type": test_type,
                "question_key": question_key,
                "total_rows": len(df),
                "total_columns": len(df.columns),
                "numeric_columns": numeric_columns,
                "missing_values": df.isnull().sum().to_dict(),
                "basic_stats": {}
            }
            
            # ìˆ«ì ì»¬ëŸ¼ì— ëŒ€í•œ ê¸°ë³¸ í†µê³„
            for col in numeric_columns:
                stats["basic_stats"][col] = {
                    "mean": float(df[col].mean()),
                    "std": float(df[col].std()),
                    "min": float(df[col].min()),
                    "max": float(df[col].max()),
                    "count": int(df[col].count())
                }
            
            # ìƒê´€ê´€ê³„ ë¶„ì„
            if len(numeric_columns) > 1:
                correlation_matrix = df[numeric_columns].corr()
                stats["correlation"] = correlation_matrix.to_dict()
            
            return {
                "success": True,
                "result": stats
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e)
            }
    
    def process_survey_data(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ì„¤ë¬¸ ë°ì´í„° ì²˜ë¦¬"""
        # ì„¤ë¬¸ ë°ì´í„° íŠ¹í™” ì²˜ë¦¬ ë¡œì§
        result = {
            "question_keys": [],
            "question_texts": {},
            "tables": {}
        }
        
        # ì§ˆë¬¸ í‚¤ ì¶”ì¶œ (ì²« ë²ˆì§¸ ì»¬ëŸ¼ì—ì„œ)
        if len(df.columns) > 0:
            first_col = df.columns[0]
            question_keys = df[first_col].dropna().unique().tolist()
            result["question_keys"] = question_keys
            
            # ê° ì§ˆë¬¸ë³„ í…Œì´ë¸” ìƒì„±
            for key in question_keys:
                if key in df[first_col].values:
                    question_data = df[df[first_col] == key]
                    result["tables"][key] = {
                        "columns": question_data.columns.tolist(),
                        "data": question_data.values.tolist()
                    }
        
        return result 

    def process_excel_file(self, file_content: bytes, file_name: str):
        """ì—‘ì…€/CSV íŒŒì¼ì˜ bytesë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜"""
        if file_name.endswith('.xlsx') or file_name.endswith('.xls'):
            return pd.read_excel(io.BytesIO(file_content))
        elif file_name.endswith('.csv'):
            return pd.read_csv(io.BytesIO(file_content))
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_name}")

    def extract_tables_from_excel(self, file_content: bytes, file_name: str = 'data.xlsx'):
        """ì—‘ì…€/CSV íŒŒì¼ì˜ bytesì—ì„œ ì§ˆë¬¸ë³„ í…Œì´ë¸” ë”•ì…”ë„ˆë¦¬ ë°˜í™˜"""
        df = self.process_excel_file(file_content, file_name)
        return self.process_survey_data(df)["tables"]

    async def analyze_table(self, table_data):
        """í…Œì´ë¸” ë°ì´í„°(ë”•ì…”ë„ˆë¦¬ ë˜ëŠ” DataFrame)ì— ëŒ€í•œ ê°„ë‹¨ í†µê³„ ë¶„ì„ ë°˜í™˜"""
        # table_dataê°€ dictì´ë©´ DataFrameìœ¼ë¡œ ë³€í™˜
        if isinstance(table_data, dict):
            df = pd.DataFrame(table_data["data"], columns=table_data["columns"])
        else:
            df = table_data
        # ìˆ«ì ì»¬ëŸ¼ì— ëŒ€í•œ ê¸°ë³¸ í†µê³„
        numeric_columns = df.select_dtypes(include=[float, int]).columns.tolist()
        stats = {}
        for col in numeric_columns:
            stats[col] = {
                "mean": float(df[col].mean()),
                "std": float(df[col].std()),
                "min": float(df[col].min()),
                "max": float(df[col].max()),
                "count": int(df[col].count())
            }
        return stats

    def load_survey_tables(self, file_content: bytes, file_name: str, sheet_name: str = "í†µê³„í‘œ"):
        df = pd.read_excel(io.BytesIO(file_content), sheet_name=sheet_name, header=None)

        pattern = r"^[A-Z]+\d*[-.]?\d*\."
        question_indices = df[df[0].astype(str).str.match(pattern)].index.tolist()

        tables = {}
        question_texts = {}
        question_keys = []
        key_counts = defaultdict(int)

        for i, start in enumerate(question_indices):
            end = question_indices[i + 1] if i + 1 < len(question_indices) else len(df)
            title = str(df.iloc[start, 0]).strip()

            match = re.match(pattern, title)
            if not match:
                continue
            base_key = match.group().rstrip(".")
            key_counts[base_key] += 1
            suffix = f"_{key_counts[base_key]}" if key_counts[base_key] > 1 else ""
            final_key = base_key + suffix
            final_key_norm = self.normalize_key(final_key) if hasattr(self, 'normalize_key') else final_key.replace("-", "_").replace(".", "_")

            question_texts[final_key_norm] = title + "(ì „ì²´ ë‹¨ìœ„ : %)"
            question_keys.append(final_key_norm)

            table = df.iloc[start + 1:end].reset_index(drop=True)

            if len(table) >= 2:
                first_header = table.iloc[0].fillna('').astype(str)
                second_header = table.iloc[1].fillna('').astype(str)

                title_text = None
                title_col_idx = None
                for idx, val in enumerate(first_header):
                    if idx > 2 and isinstance(val, str) and len(val) > 0:
                        if val not in ['ê´€ì‹¬ì—†ë‹¤', 'ë³´í†µ', 'ê´€ì‹¬ìˆë‹¤', 'í‰ê· ']:
                            title_text = val
                            title_col_idx = idx
                            break

                new_columns = []
                for idx in range(len(first_header)):
                    if idx == 0:
                        new_columns.append("ëŒ€ë¶„ë¥˜")
                    elif idx == 1:
                        new_columns.append("ì†Œë¶„ë¥˜")
                    elif idx == 2:
                        new_columns.append("ì‚¬ë¡€ìˆ˜")
                    else:
                        first_val = "" if (title_col_idx is not None and first_header.iloc[idx] == title_text) else first_header.iloc[idx]
                        combined = (first_val + " " + second_header.iloc[idx]).strip().replace('nan', '').strip()
                        new_columns.append(combined)

                table = table.drop([0, 1]).reset_index(drop=True)
                table.columns = new_columns
                table = table.dropna(axis=1, how='all')
                table = table.dropna(axis=0, how='all')
                table["ëŒ€ë¶„ë¥˜"] = table["ëŒ€ë¶„ë¥˜"].ffill()
                table = table.dropna(subset=["ëŒ€ë¶„ë¥˜", "ì‚¬ë¡€ìˆ˜"], how="all").reset_index(drop=True)
                if len(table) > 2:
                    table = table.iloc[:-1].reset_index(drop=True)

                for col in table.columns:
                    try:
                        numeric_col = pd.to_numeric(table[col], errors='coerce')
                        if numeric_col.notna().any():
                            table[col] = numeric_col.round(1)
                    except:
                        continue

                tables[final_key_norm] = table

        return {
            "tables": tables,
            "question_texts": question_texts,
            "question_keys": question_keys
        }

    def normalize_key(self, key: str) -> str:
        return key.replace("-", "_").replace(".", "_")

    def rule_based_test_type_decision(self, columns, question_text=""):
        # 1. multi_response_keywordsë¡œ ì„ì˜ ë¶„ì„ íŒë‹¨
        multi_response_keywords = [
            "1+2", "1+2+3", "ë³µìˆ˜", "ë‹¤ì¤‘", "multiple", "rank", "ranking", "ìš°ì„ ìˆœìœ„"
        ]
        text_to_check = (" ".join(columns) + " " + question_text).lower()
        if any(keyword.lower() in text_to_check for keyword in multi_response_keywords):
            return "manual"
        # 2. categorical_patternsë¡œ ft_test íŒë‹¨
        categorical_patterns = [
            r"ì „í˜€\s*ê´€ì‹¬", r"ê´€ì‹¬\s*ì—†(ë‹¤|ëŠ”)", r"ê´€ì‹¬\s*ìˆ(ë‹¤|ëŠ”)", r"ë§¤ìš°\s*ê´€ì‹¬", r"ê´€ì‹¬",
            r"ë§¤ìš°\s*ë§Œì¡±", r"ë§Œì¡±", r"ë¶ˆë§Œì¡±", r"ë§¤ìš°\s*ë¶ˆë§Œì¡±", r"ë³´í†µ",
            r"ì°¬ì„±", r"ë°˜ëŒ€", r"ë§¤ìš°\s*ì°¬ì„±", r"ë§¤ìš°\s*ë°˜ëŒ€", r"ëŒ€ì²´ë¡œ\s*ì°¬ì„±", r"ëŒ€ì²´ë¡œ\s*ë°˜ëŒ€",
            r"ë§¤ìš°\s*ì¤‘ìš”", r"ì¤‘ìš”", r"ê·¸ë‹¤ì§€\s*ì¤‘ìš”í•˜ì§€\s*ì•Š", r"ì „í˜€\s*ì¤‘ìš”í•˜ì§€\s*ì•Š",
            r"ë§¤ìš°\s*ì‹¬ê°", r"ì‹¬ê°", r"ì‹¬ê°í•˜ì§€\s*ì•Š", r"ì „í˜€\s*ì‹¬ê°í•˜ì§€\s*ì•Š",
            r"ìì£¼", r"ê°€ë”", r"ê±°ì˜\s*ì—†", r"ì „í˜€\s*ì—†",
            r"ì•ˆì „", r"ë§¤ìš°\s*ì•ˆì „", r"ìœ„í—˜", r"ë§¤ìš°\s*ìœ„í—˜",
            r"ë“¤ì–´ë³¸\s*ì ", r"ì‚¬ìš©í•œ\s*ì ", r"ê²½í—˜í–ˆ", r"ì¸ì§€",
            r"ì˜í–¥", r"ìƒê°", r"ì˜ˆì •", r"ê³„íš", r"í• \s*ê²ƒ",
            r"ë§¤ìš°", r"ì•½ê°„", r"ë³´í†µ", r"ê·¸ë‹¤ì§€", r"ì „í˜€"
        ]
        if any(any(re.search(pattern, col) for pattern in categorical_patterns) for col in columns):
            return "ft_test"
        # 3. ë‚˜ë¨¸ì§€ëŠ” chi_square
        return "chi_square"

    async def llm_test_type_decision(self, columns, question_text=""):
        # 1. multi_response_keywordsë¡œ ì„ì˜ ë¶„ì„ íŒë‹¨
        multi_response_keywords = [
            "1+2", "1+2+3", "ë³µìˆ˜", "ë‹¤ì¤‘", "multiple", "rank", "ranking", "ìš°ì„ ìˆœìœ„"
        ]
        text_to_check = (" ".join(columns) + " " + question_text).lower()
        if any(keyword.lower() in text_to_check for keyword in multi_response_keywords):
            return "manual"
        # 2. LLM í”„ë¡¬í”„íŠ¸ ìƒì„±
        TEST_TYPE_PROMPT = """
ë‹¹ì‹ ì€ í†µê³„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì•„ë˜ëŠ” ì„¤ë¬¸ ì‘ë‹µ ê²°ê³¼ í…Œì´ë¸”ì˜ ì—´ ì´ë¦„ ëª©ë¡ì…ë‹ˆë‹¤. ì´ ì—´ë“¤ì€ ì‘ë‹µìë“¤ì´ ì„ íƒí•˜ê±°ë‚˜ í‰ê°€í•œ ì„¤ë¬¸ ë¬¸í•­ì˜ ê²°ê³¼ë¡œ êµ¬ì„±ëœ í†µê³„í‘œì…ë‹ˆë‹¤.

ë‹¹ì‹ ì˜ ì„ë¬´ëŠ”, ì´ í…Œì´ë¸”ì´ **ì–´ë–¤ í†µê³„ ê²€ì •(F/T-test ë˜ëŠ” Chi-square)** ì— ì í•©í•œì§€ë¥¼ íŒë‹¨í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

ğŸ“‹ ì—´ ì´ë¦„ ëª©ë¡:
{column_names}

---
Let's think step by step

íŒë‹¨ ê¸°ì¤€:

- `ft_test` (ì—°ì†í˜• ìˆ˜ì¹˜ ì‘ë‹µ):
    - ë¬¸í•­ì´ 1~5ì  ì²™ë„, í‰ê· , ë¹„ìœ¨, ì ìˆ˜ ë“± ìˆ«ì ê¸°ë°˜ìœ¼ë¡œ ìš”ì•½ë˜ì–´ ìˆë‹¤ë©´ F-test ë˜ëŠ” T-testê°€ ì ì ˆí•©ë‹ˆë‹¤.
    - ì˜ˆì‹œ ì—´ ì´ë¦„: "í‰ê· ", "ë§Œì¡±ë„ ì ìˆ˜", "~% ë¹„ìœ¨", "5ì  ì²™ë„", "í‰ê·  ì ìˆ˜", "ê´€ì‹¬ë„ í‰ê· "
    - "ì „í˜€ ê´€ì‹¬ì´ ì—†ë‹¤", "ë§¤ìš° ê´€ì‹¬ ìˆë‹¤" ë“±ì€ ì‹¤ì œë¡œëŠ” ì„ íƒì§€ì´ì§€ë§Œ, ë¹ˆë„ë‚˜ ë¹„ìœ¨ë¡œ ìˆ˜ì¹˜í™”ë˜ì—ˆì„ ê²½ìš° â†’ ì—°ì†í˜•ìœ¼ë¡œ íŒë‹¨

- `chi_square` (ë²”ì£¼í˜• ì„ íƒ ì‘ë‹µ):
    - ë¬¸í•­ì´ ì‘ë‹µìë“¤ì´ íŠ¹ì • í•­ëª©ì„ **ì„ íƒ**í•˜ê±°ë‚˜ **ë‹¤ì¤‘ì„ íƒ**í•œ ê²°ê³¼ì¼ ê²½ìš°, ë²”ì£¼í˜• ì‘ë‹µìœ¼ë¡œ ë³´ê³  ì¹´ì´ì œê³± ê²€ì •ì´ ì í•©í•©ë‹ˆë‹¤.
    - ì˜ˆì‹œ ì—´ ì´ë¦„: "ì£¼ìš” ì´ìš©ì‹œì„¤", "ì„ íƒ ì´ìœ ", "ê°€ì¥ ë§ì´ ì„ íƒí•œ ì¥ì†Œ", "ë‹¤ì¤‘ ì‘ë‹µ"

- 'manual' (ë³µìˆ˜ ì‘ë‹µ í˜• ì§ˆë¬¸):
    - ë¬¸í•­ì˜ ë‚´ìš©ì´ ë³µìˆ˜ ì‘ë‹µ í‚¤ì›Œë“œ(1+2  ìˆœìœ„, 1+2+3 ìˆœìœ„, ë³µìˆ˜ì‘ë‹µ ë“±)ê°€ ìˆê±°ë‚˜ ì‚¬ìš©ìê°€ ë‹¨ì¼ ì‘ë‹µì´ ì•„ë‹Œ ë³µìˆ˜ ì‘ë‹µì¸ ê²½ìš° ì„ì˜ ë¶„ì„ (manual)ì´ ì í•©í•©ë‹ˆë‹¤.
        - **1ìˆœìœ„**ë§Œ ìˆì„ ê²½ìš°, ì´ëŠ” ë³µìˆ˜ ì‘ë‹µì´ ì•„ë‹Œ ë‹¨ì¼ ì‘ë‹µìœ¼ë¡œ ê°„ì£¼í•˜ì—¬ ft_test ë˜ëŠ” chi-square ì¤‘ í•˜ë‚˜ë¡œ íŒë‹¨í•´ì£¼ì„¸ìš”.
    - ì˜ˆì‹œ ë¬¸í•­: ì„œìš¸ì˜ ëŒ€ê¸°í™˜ê²½ ê°œì„ ì„ ìœ„í•´ ì„œìš¸ì‹œê°€ ê°€ì¥ ì—­ì ì„ ë‘ê³  ì¶”ì§„í•´ì•¼ í•  ë¶„ì•¼ëŠ” ë¬´ì—‡ì…ë‹ˆê¹Œ? (1+2ìˆœìœ„)(ì „ì²´ ë‹¨ìœ„ : %)

â— ì˜¤íŒ ì£¼ì˜:
- ì‘ë‹µ ì„ íƒì§€ ì´ë¦„(ì˜ˆ: "ì „í˜€ ê´€ì‹¬ ì—†ë‹¤", "ë§¤ìš° ê´€ì‹¬ ìˆë‹¤")ê°€ ì—´ ì´ë¦„ì— í¬í•¨ë˜ë”ë¼ë„, **ë¹„ìœ¨, í‰ê·  ë“±ì˜ ìˆ˜ì¹˜í˜• ìš”ì•½**ì´ë©´ `ft_test`ë¡œ ê°„ì£¼í•©ë‹ˆë‹¤.
- í…Œì´ë¸”ì´ ì „ì²´ì ìœ¼ë¡œ í‰ê· ê°’ ë˜ëŠ” %ë¹„ìœ¨ ì¤‘ì‹¬ì´ë©´ `ft_test` ì„ íƒì´ ë” ì ì ˆí•©ë‹ˆë‹¤.

---

ğŸ“Œ ë‹µë³€ í˜•ì‹: ì•„ë˜ì˜ í˜•ì‹ì²˜ëŸ¼ ì„ íƒì˜ ì´ìœ ì— ëŒ€í•´ì„œ ë‹µë³€í•˜ì§€ ë§ê³  "ì í•©í•œ í†µê³„ ê²€ì •ì˜ ë°©ë²•ë§Œ" ì¶œë ¥í•˜ì„¸ìš”.

- ë°˜ë“œì‹œ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¡œë§Œ ë‹µí•´ì£¼ì„¸ìš” (ì†Œë¬¸ì):
    - ft_test
    - chi_square

ì í•©í•œ í†µê³„ ë°©ë²•: (ft_test ë˜ëŠ” chi_square)
"""
        prompt = TEST_TYPE_PROMPT.format(column_names=", ".join(columns))
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise RuntimeError("OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
        response = await openai.AsyncOpenAI(api_key=api_key).chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ í†µê³„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.0,
            max_tokens=20
        )
        content = response.choices[0].message.content.strip().lower()
        if "chi" in content:
            return "chi_square"
        elif "ft" in content:
            return "ft_test"
        else:
            return "unknown"

def extract_guide_subjects_from_docx_bytes(docx_bytes: bytes):
    """
    DOCX íŒŒì¼ì˜ bytesë¥¼ ë°›ì•„ì„œ, ë³¸ë¬¸ê³¼ í‘œì—ì„œ ì£¼ì œ/ì§ˆë¬¸(ì„¹ì…˜ëª…, ì§ˆë¬¸ ë“±)ì„ robustí•˜ê²Œ ì¶”ì¶œí•œë‹¤.
    """
    doc = Document(BytesIO(docx_bytes))
    # 1. ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    paragraph_texts = [para.text for para in doc.paragraphs if para.text.strip() != ""]
    # 2. í‘œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    table_texts = []
    for table in doc.tables:
        for row in table.rows:
            row_text = [cell.text.strip() for cell in row.cells]
            table_texts.append("\t".join(row_text))  # íƒ­ìœ¼ë¡œ êµ¬ë¶„
    # 3. ì „ì²´ í…ìŠ¤íŠ¸ í•©ì¹˜ê¸°
    full_text = "\n".join(paragraph_texts + table_texts)
    # 4. ì£¼ì œ/ì§ˆë¬¸ robust ì¶”ì¶œ
    subjects = []
    for line in full_text.split('\n'):
        line = line.strip()
        # ìˆ«ì+ë§ˆì¹¨í‘œ/ê´„í˜¸/ì½œë¡ , â–·, -, â€¢, PART, ì†Œì œëª© ë“±
        if re.match(r'^(PART|[0-9]+[.)]|[0-9]+:|â–·|-|â€¢)', line):
            subjects.append(line)
        # ì§ˆë¬¸í˜• ë¬¸ì¥
        elif re.search(r'(ë¬´ì—‡|ì–´ë–»ê²Œ|ìˆë‚˜ìš”|ìƒê°í•˜ì‹œë‚˜ìš”|ì•Œê³  ê³„ì‹œë‚˜ìš”|ì´ìœ |ë°©ì‹|ë°©ì•ˆ|ì˜ê²¬|ê²½í—˜|ëŠë‚€ ì |ì¶”ì²œ|í‰ê°€|ì˜ë¯¸|ì •ì˜|ì°¨ì´|íŠ¹ì§•|ì¥ì |ë‹¨ì |ë¬¸ì œ|í•´ê²°|í•„ìš”|ì¤‘ìš”|ì—­í• |ê¸°ëŒ€|íš¨ê³¼|ë°©í–¥|ê³„íš|ì „ëµ|ë°©ë²•|ìˆìœ¼ì‹ ê°€ìš”|ìˆìŠµë‹ˆê¹Œ|ìˆì„ê¹Œìš”|ìˆì„ì§€)', line):
            subjects.append(line)
    return subjects 